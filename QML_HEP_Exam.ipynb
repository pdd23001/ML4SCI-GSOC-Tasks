{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Task 1- Installing Requirements"
      ],
      "metadata": {
        "id": "kZgckljnKHAZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "collapsed": true,
        "id": "Do0OF7z3rPVq",
        "outputId": "e092b274-9a61-4022-99bd-2557539c2f3e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.6/45.6 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m532.7/532.7 kB\u001b[0m \u001b[31m24.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.5/60.5 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.3/69.3 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m596.5/596.5 kB\u001b[0m \u001b[31m35.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m100.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.6/294.6 kB\u001b[0m \u001b[31m24.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m203.2/203.2 kB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.5/6.5 MB\u001b[0m \u001b[31m93.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m76.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m29.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m63.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.7/117.7 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m739.1/739.1 kB\u001b[0m \u001b[31m47.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for rpcq (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting qiskit\n",
            "  Downloading qiskit-1.4.2-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting rustworkx>=0.15.0 (from qiskit)\n",
            "  Downloading rustworkx-0.16.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
            "Requirement already satisfied: numpy<3,>=1.17 in /usr/local/lib/python3.11/dist-packages (from qiskit) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.5 in /usr/local/lib/python3.11/dist-packages (from qiskit) (1.14.1)\n",
            "Requirement already satisfied: sympy>=1.3 in /usr/local/lib/python3.11/dist-packages (from qiskit) (1.13.1)\n",
            "Collecting dill>=0.3 (from qiskit)\n",
            "  Downloading dill-0.3.9-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.8.0 in /usr/local/lib/python3.11/dist-packages (from qiskit) (2.8.2)\n",
            "Collecting stevedore>=3.0.0 (from qiskit)\n",
            "  Downloading stevedore-5.4.1-py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from qiskit) (4.12.2)\n",
            "Collecting symengine<0.14,>=0.11 (from qiskit)\n",
            "  Downloading symengine-0.13.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.0->qiskit) (1.17.0)\n",
            "Collecting pbr>=2.0.0 (from stevedore>=3.0.0->qiskit)\n",
            "  Downloading pbr-6.1.1-py2.py3-none-any.whl.metadata (3.4 kB)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy>=1.3->qiskit) (1.3.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from pbr>=2.0.0->stevedore>=3.0.0->qiskit) (75.1.0)\n",
            "Downloading qiskit-1.4.2-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.9-py3-none-any.whl (119 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.4/119.4 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading rustworkx-0.16.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m67.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading stevedore-5.4.1-py3-none-any.whl (49 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.5/49.5 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading symengine-0.13.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (49.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.7/49.7 MB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pbr-6.1.1-py2.py3-none-any.whl (108 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m109.0/109.0 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: symengine, rustworkx, pbr, dill, stevedore, qiskit\n",
            "Successfully installed dill-0.3.9 pbr-6.1.1 qiskit-1.4.2 rustworkx-0.16.0 stevedore-5.4.1 symengine-0.13.0\n",
            "Collecting pylatexenc\n",
            "  Downloading pylatexenc-2.10.tar.gz (162 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m162.6/162.6 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: pylatexenc\n",
            "  Building wheel for pylatexenc (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pylatexenc: filename=pylatexenc-2.10-py3-none-any.whl size=136816 sha256=9e33e364bd66db7d50e0288e43297cf82990d5ccb4f470ccdfb648333393828d\n",
            "  Stored in directory: /root/.cache/pip/wheels/b1/7a/33/9fdd892f784ed4afda62b685ae3703adf4c91aa0f524c28f03\n",
            "Successfully built pylatexenc\n",
            "Installing collected packages: pylatexenc\n",
            "Successfully installed pylatexenc-2.10\n"
          ]
        }
      ],
      "source": [
        "!pip install --quiet cirq\n",
        "!pip install qiskit\n",
        "!pip install pylatexenc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CClBiUR3yPMD"
      },
      "source": [
        "# Task 1 - Required Imports (Run Before Task-1 Part-1 and 2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k0JkmYs-yXyC"
      },
      "source": [
        "**Cirq:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "KBujERRwr_oi"
      },
      "outputs": [],
      "source": [
        "import cirq\n",
        "import cirq_google\n",
        "from cirq.contrib.svg import SVGCircuit"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gByKPJsqygR9"
      },
      "source": [
        "**Qiskit:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "VfzF051gyWuR"
      },
      "outputs": [],
      "source": [
        "from qiskit.circuit import QuantumCircuit, Parameter"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zNrx7Vqbyz9E"
      },
      "source": [
        "**Misc:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "B3qJP07Cyzhq"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pylatexenc\n",
        "import matplotlib"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aH9dGoCHxbau"
      },
      "source": [
        "# Task 1 - Part 1 (Using Cirq)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "3EXs1b2qseLZ"
      },
      "outputs": [],
      "source": [
        "qubits=[cirq.LineQubit(i) for i in range(5)] #I am using line qubits. Here I am creating them and storing them in a list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 311
        },
        "id": "OtZ_kQvEs44R",
        "outputId": "f40c62d9-d804-4c9f-b665-c9a0ebada03c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<cirq.contrib.svg.svg.SVGCircuit at 0x79fce87c5950>"
            ],
            "image/svg+xml": "<svg xmlns=\"http://www.w3.org/2000/svg\" width=\"527.05328125\" height=\"290.0\"><line x1=\"30.0\" x2=\"497.05328125\" y1=\"45.0\" y2=\"45.0\" stroke=\"#1967d2\" stroke-width=\"1\" /><line x1=\"30.0\" x2=\"497.05328125\" y1=\"95.0\" y2=\"95.0\" stroke=\"#1967d2\" stroke-width=\"1\" /><line x1=\"30.0\" x2=\"497.05328125\" y1=\"145.0\" y2=\"145.0\" stroke=\"#1967d2\" stroke-width=\"1\" /><line x1=\"30.0\" x2=\"497.05328125\" y1=\"195.0\" y2=\"195.0\" stroke=\"#1967d2\" stroke-width=\"1\" /><line x1=\"30.0\" x2=\"497.05328125\" y1=\"245.0\" y2=\"245.0\" stroke=\"#1967d2\" stroke-width=\"1\" /><line x1=\"360.0\" x2=\"477.05328125\" y1=\"5.0\" y2=\"5.0\" stroke=\"black\" stroke-width=\"1\" /><line x1=\"360.0\" x2=\"477.05328125\" y1=\"285.0\" y2=\"285.0\" stroke=\"black\" stroke-width=\"1\" /><line x1=\"150.0\" x2=\"150.0\" y1=\"45.0\" y2=\"95.0\" stroke=\"black\" stroke-width=\"3\" /><line x1=\"210.0\" x2=\"210.0\" y1=\"95.0\" y2=\"145.0\" stroke=\"black\" stroke-width=\"3\" /><line x1=\"270.0\" x2=\"270.0\" y1=\"145.0\" y2=\"195.0\" stroke=\"black\" stroke-width=\"3\" /><line x1=\"330.0\" x2=\"330.0\" y1=\"195.0\" y2=\"245.0\" stroke=\"black\" stroke-width=\"3\" /><line x1=\"380.0\" x2=\"380.0\" y1=\"45.0\" y2=\"245.0\" stroke=\"black\" stroke-width=\"3\" /><line x1=\"360.0\" x2=\"360.0\" y1=\"5.0\" y2=\"15.0\" stroke=\"black\" stroke-width=\"3\" /><line x1=\"477.05328125\" x2=\"477.05328125\" y1=\"5.0\" y2=\"15.0\" stroke=\"black\" stroke-width=\"3\" /><line x1=\"360.0\" x2=\"360.0\" y1=\"275.0\" y2=\"285.0\" stroke=\"black\" stroke-width=\"3\" /><line x1=\"477.05328125\" x2=\"477.05328125\" y1=\"275.0\" y2=\"285.0\" stroke=\"black\" stroke-width=\"3\" /><rect x=\"10.0\" y=\"25.0\" width=\"40\" height=\"40\" stroke=\"black\" fill=\"white\" stroke-width=\"0\" /><text x=\"30.0\" y=\"45.0\" dominant-baseline=\"middle\" text-anchor=\"middle\" font-size=\"14px\" font-family=\"sans\\-serif:style=normal:variant=normal:weight=normal:stretch=normal:size=10.0\">0: </text><rect x=\"10.0\" y=\"75.0\" width=\"40\" height=\"40\" stroke=\"black\" fill=\"white\" stroke-width=\"0\" /><text x=\"30.0\" y=\"95.0\" dominant-baseline=\"middle\" text-anchor=\"middle\" font-size=\"14px\" font-family=\"sans\\-serif:style=normal:variant=normal:weight=normal:stretch=normal:size=10.0\">1: </text><rect x=\"10.0\" y=\"125.0\" width=\"40\" height=\"40\" stroke=\"black\" fill=\"white\" stroke-width=\"0\" /><text x=\"30.0\" y=\"145.0\" dominant-baseline=\"middle\" text-anchor=\"middle\" font-size=\"14px\" font-family=\"sans\\-serif:style=normal:variant=normal:weight=normal:stretch=normal:size=10.0\">2: </text><rect x=\"10.0\" y=\"175.0\" width=\"40\" height=\"40\" stroke=\"black\" fill=\"white\" stroke-width=\"0\" /><text x=\"30.0\" y=\"195.0\" dominant-baseline=\"middle\" text-anchor=\"middle\" font-size=\"14px\" font-family=\"sans\\-serif:style=normal:variant=normal:weight=normal:stretch=normal:size=10.0\">3: </text><rect x=\"10.0\" y=\"225.0\" width=\"40\" height=\"40\" stroke=\"black\" fill=\"white\" stroke-width=\"0\" /><text x=\"30.0\" y=\"245.0\" dominant-baseline=\"middle\" text-anchor=\"middle\" font-size=\"14px\" font-family=\"sans\\-serif:style=normal:variant=normal:weight=normal:stretch=normal:size=10.0\">4: </text><rect x=\"70.0\" y=\"25.0\" width=\"40\" height=\"40\" stroke=\"black\" fill=\"white\" stroke-width=\"1\" /><text x=\"90.0\" y=\"45.0\" dominant-baseline=\"middle\" text-anchor=\"middle\" font-size=\"18px\" font-family=\"sans\\-serif:style=normal:variant=normal:weight=normal:stretch=normal:size=10.0\">H</text><rect x=\"70.0\" y=\"75.0\" width=\"40\" height=\"40\" stroke=\"black\" fill=\"white\" stroke-width=\"1\" /><text x=\"90.0\" y=\"95.0\" dominant-baseline=\"middle\" text-anchor=\"middle\" font-size=\"18px\" font-family=\"sans\\-serif:style=normal:variant=normal:weight=normal:stretch=normal:size=10.0\">H</text><rect x=\"70.0\" y=\"125.0\" width=\"40\" height=\"40\" stroke=\"black\" fill=\"white\" stroke-width=\"1\" /><text x=\"90.0\" y=\"145.0\" dominant-baseline=\"middle\" text-anchor=\"middle\" font-size=\"18px\" font-family=\"sans\\-serif:style=normal:variant=normal:weight=normal:stretch=normal:size=10.0\">H</text><rect x=\"70.0\" y=\"175.0\" width=\"40\" height=\"40\" stroke=\"black\" fill=\"white\" stroke-width=\"1\" /><text x=\"90.0\" y=\"195.0\" dominant-baseline=\"middle\" text-anchor=\"middle\" font-size=\"18px\" font-family=\"sans\\-serif:style=normal:variant=normal:weight=normal:stretch=normal:size=10.0\">H</text><rect x=\"70.0\" y=\"225.0\" width=\"40\" height=\"40\" stroke=\"black\" fill=\"white\" stroke-width=\"1\" /><text x=\"90.0\" y=\"245.0\" dominant-baseline=\"middle\" text-anchor=\"middle\" font-size=\"18px\" font-family=\"sans\\-serif:style=normal:variant=normal:weight=normal:stretch=normal:size=10.0\">H</text><circle cx=\"150.0\" cy=\"45.0\" r=\"10.0\" /><rect x=\"130.0\" y=\"75.0\" width=\"40\" height=\"40\" stroke=\"black\" fill=\"white\" stroke-width=\"1\" /><text x=\"150.0\" y=\"95.0\" dominant-baseline=\"middle\" text-anchor=\"middle\" font-size=\"18px\" font-family=\"sans\\-serif:style=normal:variant=normal:weight=normal:stretch=normal:size=10.0\">X</text><circle cx=\"210.0\" cy=\"95.0\" r=\"10.0\" /><rect x=\"190.0\" y=\"125.0\" width=\"40\" height=\"40\" stroke=\"black\" fill=\"white\" stroke-width=\"1\" /><text x=\"210.0\" y=\"145.0\" dominant-baseline=\"middle\" text-anchor=\"middle\" font-size=\"18px\" font-family=\"sans\\-serif:style=normal:variant=normal:weight=normal:stretch=normal:size=10.0\">X</text><circle cx=\"270.0\" cy=\"145.0\" r=\"10.0\" /><rect x=\"250.0\" y=\"175.0\" width=\"40\" height=\"40\" stroke=\"black\" fill=\"white\" stroke-width=\"1\" /><text x=\"270.0\" y=\"195.0\" dominant-baseline=\"middle\" text-anchor=\"middle\" font-size=\"18px\" font-family=\"sans\\-serif:style=normal:variant=normal:weight=normal:stretch=normal:size=10.0\">X</text><circle cx=\"330.0\" cy=\"195.0\" r=\"10.0\" /><rect x=\"310.0\" y=\"225.0\" width=\"40\" height=\"40\" stroke=\"black\" fill=\"white\" stroke-width=\"1\" /><text x=\"330.0\" y=\"245.0\" dominant-baseline=\"middle\" text-anchor=\"middle\" font-size=\"18px\" font-family=\"sans\\-serif:style=normal:variant=normal:weight=normal:stretch=normal:size=10.0\">X</text><text x=\"380.0\" y=\"48.0\" dominant-baseline=\"middle\" text-anchor=\"middle\" font-size=\"40px\" font-family=\"sans\\-serif:style=normal:variant=normal:weight=normal:stretch=normal:size=10.0\">×</text><text x=\"380.0\" y=\"248.0\" dominant-baseline=\"middle\" text-anchor=\"middle\" font-size=\"40px\" font-family=\"sans\\-serif:style=normal:variant=normal:weight=normal:stretch=normal:size=10.0\">×</text><rect x=\"400.0\" y=\"175.0\" width=\"67.05328125\" height=\"40\" stroke=\"black\" fill=\"white\" stroke-width=\"1\" /><text x=\"433.526640625\" y=\"195.0\" dominant-baseline=\"middle\" text-anchor=\"middle\" font-size=\"14px\" font-family=\"sans\\-serif:style=normal:variant=normal:weight=normal:stretch=normal:size=10.0\">Rx(0.5π)</text></svg>"
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "circuit=cirq.Circuit() #creating empty circuit\n",
        "circuit.append(cirq.H.on_each(qubits))               #Applying hadamard on all of them\n",
        "\n",
        "circuit.append([cirq.CNOT(qubits[0],qubits[1]),      #Here I am appending a list of CNOT gate objects to the circuit. Each CNOT gate has first qubit controla and second as target\n",
        "                cirq.CNOT(qubits[1],qubits[2]),\n",
        "                cirq.CNOT(qubits[2],qubits[3]),\n",
        "                cirq.CNOT(qubits[3],qubits[4])])\n",
        "\n",
        "circuit.append(cirq.SWAP(qubits[0],qubits[4]))       #This is the swap gate for first and last qubit\n",
        "\n",
        "circuit.append(cirq.rx(np.pi/2)(qubits[3]))          #Here I am applying the rx gate i.e. rotate about x-axis gate.\n",
        "\n",
        "SVGCircuit(circuit)                                  #I have used SVGCircuit for showing a good picture of the circuit. However print(circuit) would also give an accurate representation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kDqJZJpzzQ43"
      },
      "source": [
        "# Task 1- Part 2 (Using Qiskit)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "QTt4xqzBzn7U",
        "outputId": "eff199d9-720e-40b6-b4af-04bf5dbd67f4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "        ┌───┐                 \n",
            "q_0: ───┤ H ├────X────────────\n",
            "     ┌──┴───┴──┐ │            \n",
            "q_1: ┤ Rx(π/3) ├─┼──X─────────\n",
            "     └──┬───┬──┘ │  │         \n",
            "q_2: ───┤ H ├────X──┼─────────\n",
            "        ├───┤    │  │         \n",
            "q_3: ───┤ H ├────┼──X─────────\n",
            "        ├───┤    │  │ ┌───┐┌─┐\n",
            "q_4: ───┤ H ├────■──■─┤ H ├┤M├\n",
            "        └───┘         └───┘└╥┘\n",
            "c: 1/═══════════════════════╩═\n",
            "                            0 \n"
          ]
        }
      ],
      "source": [
        "circuit=QuantumCircuit(5,1)\n",
        "circuit.h([0,2,3,4])      #Applying hadamard to first, third and fourth qubit. We laso applied it to the 5th qubit as we are using that as the ancilla in our swap test\n",
        "circuit.rx(np.pi/3,1)     #rotating second qubit by pi/3 around X\n",
        "circuit.cswap(4,0,2)      #Fredkin on 1st and 3rd with last qubit as control\n",
        "circuit.cswap(4,1,3)      #Fredkin on 2nd and 4th with last qubit as control\n",
        "circuit.h(4)              #Applying hadamard on ancilla again\n",
        "circuit.measure(4,0)      #measuring ancilla (Greater probability of 0 means greater similarity. We can find probability through a sampler)\n",
        "print(circuit)\n",
        "#Here we can also use circuit.draw('mpl') for a beautified version"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8xqJ17-t5ovO"
      },
      "source": [
        "# Task-2 Installing Requirements (Run before going to Task 2- Required Imports)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "collapsed": true,
        "id": "oOzFo-xx6XMr",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "815c89f3-723e-487c-9441-f49bf46a5208"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting energyflow\n",
            "  Downloading energyflow-1.4.0-py3-none-any.whl.metadata (5.6 kB)\n",
            "Requirement already satisfied: h5py!=3.11.0,>=2.9.0 in /usr/local/lib/python3.11/dist-packages (from energyflow) (3.13.0)\n",
            "Requirement already satisfied: numpy>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from energyflow) (1.26.4)\n",
            "Collecting wasserstein>=1.0.1 (from energyflow)\n",
            "  Downloading wasserstein-1.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\n",
            "Collecting wurlitzer>=2.0.0 (from wasserstein>=1.0.1->energyflow)\n",
            "  Downloading wurlitzer-3.1.1-py3-none-any.whl.metadata (2.5 kB)\n",
            "Downloading energyflow-1.4.0-py3-none-any.whl (700 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m700.8/700.8 kB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading wasserstein-1.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (502 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m502.2/502.2 kB\u001b[0m \u001b[31m30.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading wurlitzer-3.1.1-py3-none-any.whl (8.6 kB)\n",
            "Installing collected packages: wurlitzer, wasserstein, energyflow\n",
            "Successfully installed energyflow-1.4.0 wasserstein-1.1.0 wurlitzer-3.1.1\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m112.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m79.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m58.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m46.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n",
            "Collecting torch-geometric\n",
            "  Downloading torch_geometric-2.6.1-py3-none-any.whl.metadata (63 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.1/63.1 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (3.11.14)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (2025.3.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (3.1.6)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (1.26.4)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (5.9.5)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (3.2.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (4.67.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (6.2.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (0.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (1.18.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch-geometric) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (2025.1.31)\n",
            "Downloading torch_geometric-2.6.1-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m29.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torch-geometric\n",
            "Successfully installed torch-geometric-2.6.1\n",
            "Collecting torch-cluster\n",
            "  Downloading torch_cluster-1.6.3.tar.gz (54 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.5/54.5 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from torch-cluster) (1.14.1)\n",
            "Requirement already satisfied: numpy<2.3,>=1.23.5 in /usr/local/lib/python3.11/dist-packages (from scipy->torch-cluster) (1.26.4)\n",
            "Building wheels for collected packages: torch-cluster\n",
            "  Building wheel for torch-cluster (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torch-cluster: filename=torch_cluster-1.6.3-cp311-cp311-linux_x86_64.whl size=2057700 sha256=7e82e3577b5160d1595451db5e9afd9fba2b0fb5ab82466e1fb3fecabe8c5bde\n",
            "  Stored in directory: /root/.cache/pip/wheels/ef/de/7d/a4211822af99147b93800e9e204f0be21294e3c0b95b3b861a\n",
            "Successfully built torch-cluster\n",
            "Installing collected packages: torch-cluster\n",
            "Successfully installed torch-cluster-1.6.3\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.14.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install energyflow           #for loading quark-gluon data\n",
        "!pip install torch\n",
        "!pip install torch-geometric\n",
        "!pip install torch-cluster        #this might take 10-15 mins to install. Ateast that was the case on colab\n",
        "!pip install scikit-learn\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RSaM7YUJ7WFT"
      },
      "source": [
        "# Task-2 Required Imports (Run before running the Task-2 Graph Based ...)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "oNGfgocI7aYy"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional\n",
        "from torch_geometric.loader import DataLoader\n",
        "from torch_geometric.data import Data\n",
        "from torch_geometric.nn import global_mean_pool, global_max_pool, GATConv, EdgeConv\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch_cluster import knn_graph\n",
        "import energyflow"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lfuqcqlx8fQO"
      },
      "source": [
        "# Task-2 Graph Based Quark-Gluon Classification Architectures"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NLqE2Sg_edyz"
      },
      "source": [
        "The objective of task-2 is to explore two graph based architectures for classification of quarks and gluon jets in the dataset provided in the task.\n",
        "\n",
        "I have chosen the following graph based architectures:\n",
        "\n",
        "\n",
        "1.   Graph Attention Networks (**GAT**)\n",
        "2.   Dynamic Graph Convolutional Neural Networks (**DGCNN**)\n",
        "\n",
        "\n",
        "**Before I explore these models, I would like to explain how I projected this dataset to a set of interconnected nodes and edges-**\n",
        "\n",
        "Each jet in the dataset has been considered as a graph (i.e set of interconnected nodes and edges). There are 100,000 jets in total. Thus we would have 100,000 graphs.\n",
        "\n",
        "Every jet is a 2-D array in the dataset. It is an array of particles. Every particle in the jet is a 1-D array consisting of 4 features (pt, rapidity, azimuthal angle, and pdgid)\n",
        "\n",
        "Thus I am considering every particle in the jet as a node inside the graph. The features are basically attributes of that node. Since features can vary over several orders of magnitude, I also normalize each feature which helps in stabilizing training and ensures that all features contribute comparably when computing distances. The dataset is also padded with zeros as particles for jets that have lesser than the maximum number of particles. However, these don't provide any information thus they are removed.\n",
        "\n",
        "Now coming to edges, I constructed edges for each node by using the k-Nearest Neighbors approach. For every node, I connected it to its k-NN based on euclidean distance in the feature space.  In the current case, using k=16 neighbors typically provides sufficient connectivity for the GAT or DGCNN to learn meaningful representations without overwhelming the model with too many edges.\n",
        "\n",
        "Also, in the k-NN graph, I have not included self-loops (edges from a node to itself) since they do not provide additional relational information.\n",
        "\n",
        "Additionaly, GAT uses a fixed (static) graph using the raw features. In case of DGCNN, the benefit is in dynamically recomputing the k-NN graph at each layer to capture evolving relationships as the node features get updated.\n",
        "\n",
        "Once the nodes are processed by the network layers, I have applied global pooling to aggregate node-level information into a single, fixed-size graph-level representation. This aggregated vector is used for the final classification task.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ycMkTtCA5Hcq"
      },
      "source": [
        "The **create_graph_list** function below shows what I have written above.\n",
        "\n",
        "It loads the data set as two arrays-\n",
        "\n",
        "A 3-D array (X) of shape (N,M,d) where N is the number of jets, M is max number of particles per jet and d is the number of features per particle.\n",
        "\n",
        "A 1-D array (Y) of the output labels (Quark/Gluon) for every jet\n",
        "\n",
        "Then performs normalization, removing padding, k-NN graph finding.\n",
        "It returns a list of PyG objects which are basically jets wrapped up with their respective labels. Run the cell below to preprocess the dataset and get it into appropriate for running through the models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "A47s0gnU49De"
      },
      "outputs": [],
      "source": [
        "def create_graph_list(k):\n",
        "\n",
        "    X,Y=energyflow.qg_jets.load(num_data=100000, pad=True, ncol=4, generator='pythia',with_bc=False, cache_dir='~/.energyflow')     #loading the QG dataset X is 3-D array and Y is 1-D\n",
        "\n",
        "    data_list=[]\n",
        "\n",
        "    for i in range(X.shape[0]):                                      #Iterating through the jets to create a graph for each jet\n",
        "        x=torch.tensor(X[i], dtype=torch.float)                      # shape: (M, d)\n",
        "        x=(x-x.mean(dim=0))/x.std(dim=0)                             #Normalizing the features across particles (for each feature column).\n",
        "        mask=(x.abs().sum(dim=1)>1e-8)                               #Removing padded particles (all input features 0)\n",
        "        x=x[mask]\n",
        "        edge_index=knn_graph(x,k=k,loop=False)                       #Finds k-NN for every node/particle in the graph/jet in feature space (uses Euclidean distance)\n",
        "        label=torch.tensor([Y[i]],dtype=torch.long)                  #I am wrapping the label as a tensor as well to add to the PyG data object\n",
        "        data_list.append(Data(x=x,edge_index=edge_index,y=label))    #creating a PyG data object and appending it to a list of graphs. Each graph/jet is a PyG object\n",
        "\n",
        "    return data_list"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7SrMIYYO9dT_"
      },
      "source": [
        "**GAT:**\n",
        "\n",
        "GAT layers learn to weigh each neighbor’s influence differently by computing attention scores. Multiple attention heads allow the model to capture different types of interactions. This is why I have chose GAT as one of my architectures as not all particles contribute equally to identifying a jet as quark- or gluon-initiated. GAT’s attention mechanism lets the network focus more on the most informative particle interactions. The heads enable the network to learn various aspects of the relationships between particles, which is crucial given the complex structure of jets.\n",
        "\n",
        "\n",
        "After processing through three GAT layers, I have used global mean pooling to aggregate node features into a single graph-level feature, which is then fed into a fully connected layer for classification.\n",
        "\n",
        "Below is the implementation of the GAT. Please run the cell so that it can be used later."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "collapsed": true,
        "id": "A2wQgUxT8jH8"
      },
      "outputs": [],
      "source": [
        "class QG_GAT(nn.Module):\n",
        "\n",
        "  def __init__(self, in_channels, hidden_channels, out_channels=2, heads=4):\n",
        "\n",
        "    super().__init__()\n",
        "    #3 processing layers\n",
        "    self.layer_1=GATConv(in_channels, hidden_channels, heads=heads, concat=True)             #'heads' attention heads and their outputs concatenated. Outputs are feature vectors of hidden channels\n",
        "    self.layer_2=GATConv(hidden_channels*heads, hidden_channels, heads=heads, concat=True)   #Thus input channels is hidden_channels*heads. Same heads, concatenation and output feature vector size as as before\n",
        "    self.layer_3=GATConv(hidden_channels*heads, hidden_channels, heads=1, concat=False)      #1 attention head only now therefore the fc layer gets a feature vector of hidden_channels size\n",
        "\n",
        "    self.fc=nn.Linear(hidden_channels, out_channels)                                         #maps to final outputs\n",
        "\n",
        "  def forward(self, x, edge_index, batch):\n",
        "\n",
        "    x=functional.elu(self.layer_1(x, edge_index))                                            #Applying GAT layers with ELU activation.\n",
        "    x=functional.elu(self.layer_2(x, edge_index))\n",
        "    x=functional.elu(self.layer_3(x, edge_index))\n",
        "\n",
        "    x=global_mean_pool(x, batch)\n",
        "\n",
        "    return self.fc(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pu-MHVwe_QiR"
      },
      "source": [
        "**DGCNN-**\n",
        "\n",
        "Instead of using a fixed graph, DGCNN recomputes the graph (using k-NN) at every layer. This means that after each layer, as the node features change, the graph’s connectivity is updated accordingly. Thus, I used DGCNN the network learns, the optimal relationships between particles can change. DGCNN’s dynamic graph construction allows the network to update these relationships at every layer, capturing higher-level, context-dependent interactions.\n",
        "\n",
        "DGCNN uses EdgeConv layers that consider both a node and its neighbors. EdgeConv layers are particularly good at learning local structures, which is important because the spatial and energy distributions of particles in a jet are key to distinguishing between quark and gluon jets.\n",
        "\n",
        "After several EdgeConv layers, global pooling (both max and mean) aggregates the node features into a graph-level feature vector, which is then passed through a fully connected layer to yield class scores.\n",
        "\n",
        "\n",
        "Below is the implementation of the DGCNN. Please run the cell so that it can be used later."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "Oo8CTrVkV1jm"
      },
      "outputs": [],
      "source": [
        "class QG_DGCNN(nn.Module):\n",
        "\n",
        "  def __init__(self, in_channels, hidden_channels, out_channels=2, k=16):\n",
        "\n",
        "    super().__init__()\n",
        "    self.k=k\n",
        "    #4 processing layers\n",
        "    self.conv_layer_1=EdgeConv(nn.Sequential(nn.Linear(2*in_channels, hidden_channels),nn.ReLU(), nn.Linear(hidden_channels,hidden_channels)))            #Input dimension is 2*in_channels because features of a node and its neighbor are concatenated. Outputs a feature vector of size hidden channels\n",
        "    self.conv_layer_2=EdgeConv(nn.Sequential(nn.Linear(2*hidden_channels, hidden_channels), nn.ReLU(), nn.Linear(hidden_channels,hidden_channels)))       #Input dimension is 2*hidden_channels (from previous layer's output)\n",
        "    self.conv_layer_3=EdgeConv(nn.Sequential(nn.Linear(2*hidden_channels, hidden_channels), nn.ReLU(), nn.Linear(hidden_channels,hidden_channels)))\n",
        "    self.conv_layer_4=EdgeConv(nn.Sequential(nn.Linear(2*hidden_channels, hidden_channels), nn.ReLU(), nn.Linear(hidden_channels,hidden_channels)))\n",
        "\n",
        "    self.conv_layer_list=[self.conv_layer_1, self.conv_layer_2, self.conv_layer_3, self.conv_layer_4]                                #Save all EdgeConv layers in a list for iteration in the forward pass\n",
        "\n",
        "    self.fc=nn.Linear(hidden_channels*2, out_channels)                                                                               #Takes the concatenated output from global max and mean pooling (2*hidden_channels) and maps it to out_channels classes\n",
        "\n",
        "\n",
        "  def forward(self, x, edge_index, batch):\n",
        "\n",
        "    for layer in self.conv_layer_list:\n",
        "      edge_index = knn_graph(x, k=self.k, batch=batch, loop=False)                 #Dynamically computing the k-NN graph based on the current node features\n",
        "      x=layer(x, edge_index)                                                       #Updating the node features using the current EdgeConv layer\n",
        "\n",
        "    x_max=global_max_pool(x, batch)\n",
        "    x_mean=global_mean_pool(x, batch)\n",
        "    x=torch.cat([x_max, x_mean], dim=1)                                            #Concatenating both pooled representations to form a graph-level feature vector\n",
        "\n",
        "    return self.fc(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-C_rh9O6CIBe"
      },
      "source": [
        "Below are functions for training and testing. These are used for a single epoch and are iteratively run later in main() for multiple (20) epochs\n",
        "\n",
        "Please run the cell so that they can be used later"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "SBqpG_CsV1R-"
      },
      "outputs": [],
      "source": [
        "def train_epoch(model, loader, optimizer, device):\n",
        "\n",
        "    model.train()                                                 #Putting model in train mode\n",
        "    total_loss=0\n",
        "    for batch in loader:                                          #going through the batches here\n",
        "        batch=batch.to(device)\n",
        "        optimizer.zero_grad()                                     #I am clearing all previous gradient computations here\n",
        "        forward=model(batch.x, batch.edge_index, batch.batch)     #Running the forward method in the given model here\n",
        "        loss=functional.cross_entropy(forward, batch.y)           #Calculating cross entropy loss\n",
        "        loss.backward()                                           #Backpropagation\n",
        "        optimizer.step()                                          #Updating the parameters\n",
        "        total_loss+=loss.item()*batch.num_graphs                  #Loss for each batch is weighted with the number of graphs in each batch\n",
        "    return total_loss/len(loader.dataset)                         #Returns the average loss per graph/jet for an epoch\n",
        "\n",
        "def test_epoch(model, loader, device):\n",
        "\n",
        "    model.eval()                                                  #Putting model in evaluation mode\n",
        "    correct=0\n",
        "    for batch in loader:\n",
        "        batch=batch.to(device)\n",
        "        with torch.no_grad():                                     #Disabling gradient computation for evaluation\n",
        "            forward=model(batch.x, batch.edge_index, batch.batch) #Running the forward method in the model here\n",
        "            prediction=forward.argmax(dim=1)                      #Predicted class\n",
        "            correct+=prediction.eq(batch.y).sum().item()          #Number of correctly predicted graphs\n",
        "    return correct/len(loader.dataset)                            #Returning accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-_dZXGRaDJDN"
      },
      "source": [
        "Below is the main functions that trains and tests both these models on 'num_epochs' epochs. I am using 20 as num_epochs when I call main.\n",
        "\n",
        "Please run the cell to train, test and generate best accuracy values when predicting with both models.\n",
        "\n",
        "I only split the dataset into training and test sets for simplicity, but a validation set can be put in as well with a different split."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "in7A8REnV7BC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "cf7aee2e-2b44-47a8-aa3e-377a233f4263"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch_geometric/warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
            "  warnings.warn(message)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 01\n",
            "GAT Loss: 0.4998, GAT Train Accuaracy: 0.7719, GAT Test Accuracy: 0.7765\n",
            "DGCNN Loss: 0.5007, DGCNN Train Accuaracy: 0.7752, DGCNN Test Accuracy: 0.7788\n",
            "\n",
            "Epoch: 02\n",
            "GAT Loss: 0.4904, GAT Train Accuaracy: 0.7735, GAT Test Accuracy: 0.7781\n",
            "DGCNN Loss: 0.4853, DGCNN Train Accuaracy: 0.7768, DGCNN Test Accuracy: 0.7811\n",
            "\n",
            "Epoch: 03\n",
            "GAT Loss: 0.4878, GAT Train Accuaracy: 0.7759, GAT Test Accuracy: 0.7804\n",
            "DGCNN Loss: 0.4817, DGCNN Train Accuaracy: 0.7763, DGCNN Test Accuracy: 0.7784\n",
            "\n",
            "Epoch: 04\n",
            "GAT Loss: 0.4863, GAT Train Accuaracy: 0.7758, GAT Test Accuracy: 0.7798\n",
            "DGCNN Loss: 0.4786, DGCNN Train Accuaracy: 0.7763, DGCNN Test Accuracy: 0.7772\n",
            "\n",
            "Epoch: 05\n",
            "GAT Loss: 0.4841, GAT Train Accuaracy: 0.7765, GAT Test Accuracy: 0.7802\n",
            "DGCNN Loss: 0.4768, DGCNN Train Accuaracy: 0.7808, DGCNN Test Accuracy: 0.7849\n",
            "\n",
            "Epoch: 06\n",
            "GAT Loss: 0.4824, GAT Train Accuaracy: 0.7754, GAT Test Accuracy: 0.7814\n",
            "DGCNN Loss: 0.4741, DGCNN Train Accuaracy: 0.7838, DGCNN Test Accuracy: 0.7893\n",
            "\n",
            "Epoch: 07\n",
            "GAT Loss: 0.4828, GAT Train Accuaracy: 0.7772, GAT Test Accuracy: 0.7817\n",
            "DGCNN Loss: 0.4719, DGCNN Train Accuaracy: 0.7789, DGCNN Test Accuracy: 0.7832\n",
            "\n",
            "Epoch: 08\n",
            "GAT Loss: 0.4820, GAT Train Accuaracy: 0.7783, GAT Test Accuracy: 0.7843\n",
            "DGCNN Loss: 0.4694, DGCNN Train Accuaracy: 0.7843, DGCNN Test Accuracy: 0.7857\n",
            "\n",
            "Epoch: 09\n",
            "GAT Loss: 0.4816, GAT Train Accuaracy: 0.7734, GAT Test Accuracy: 0.7794\n",
            "DGCNN Loss: 0.4675, DGCNN Train Accuaracy: 0.7870, DGCNN Test Accuracy: 0.7903\n",
            "\n",
            "Epoch: 10\n",
            "GAT Loss: 0.4806, GAT Train Accuaracy: 0.7764, GAT Test Accuracy: 0.7813\n",
            "DGCNN Loss: 0.4653, DGCNN Train Accuaracy: 0.7908, DGCNN Test Accuracy: 0.7947\n",
            "\n",
            "Epoch: 11\n",
            "GAT Loss: 0.4804, GAT Train Accuaracy: 0.7791, GAT Test Accuracy: 0.7847\n",
            "DGCNN Loss: 0.4637, DGCNN Train Accuaracy: 0.7871, DGCNN Test Accuracy: 0.7901\n",
            "\n",
            "Epoch: 12\n",
            "GAT Loss: 0.4796, GAT Train Accuaracy: 0.7795, GAT Test Accuracy: 0.7854\n",
            "DGCNN Loss: 0.4625, DGCNN Train Accuaracy: 0.7877, DGCNN Test Accuracy: 0.7909\n",
            "\n",
            "Epoch: 13\n",
            "GAT Loss: 0.4793, GAT Train Accuaracy: 0.7784, GAT Test Accuracy: 0.7839\n",
            "DGCNN Loss: 0.4615, DGCNN Train Accuaracy: 0.7899, DGCNN Test Accuracy: 0.7932\n",
            "\n",
            "Epoch: 14\n",
            "GAT Loss: 0.4790, GAT Train Accuaracy: 0.7795, GAT Test Accuracy: 0.7850\n",
            "DGCNN Loss: 0.4592, DGCNN Train Accuaracy: 0.7868, DGCNN Test Accuracy: 0.7876\n",
            "\n",
            "Epoch: 15\n",
            "GAT Loss: 0.4792, GAT Train Accuaracy: 0.7786, GAT Test Accuracy: 0.7836\n",
            "DGCNN Loss: 0.4584, DGCNN Train Accuaracy: 0.7922, DGCNN Test Accuracy: 0.7937\n",
            "\n",
            "Epoch: 16\n",
            "GAT Loss: 0.4785, GAT Train Accuaracy: 0.7782, GAT Test Accuracy: 0.7835\n",
            "DGCNN Loss: 0.4567, DGCNN Train Accuaracy: 0.7933, DGCNN Test Accuracy: 0.7949\n",
            "\n",
            "Epoch: 17\n",
            "GAT Loss: 0.4785, GAT Train Accuaracy: 0.7798, GAT Test Accuracy: 0.7824\n",
            "DGCNN Loss: 0.4556, DGCNN Train Accuaracy: 0.7862, DGCNN Test Accuracy: 0.7860\n",
            "\n",
            "Epoch: 18\n",
            "GAT Loss: 0.4775, GAT Train Accuaracy: 0.7786, GAT Test Accuracy: 0.7826\n",
            "DGCNN Loss: 0.4549, DGCNN Train Accuaracy: 0.7958, DGCNN Test Accuracy: 0.7933\n",
            "\n",
            "Epoch: 19\n",
            "GAT Loss: 0.4773, GAT Train Accuaracy: 0.7786, GAT Test Accuracy: 0.7854\n",
            "DGCNN Loss: 0.4531, DGCNN Train Accuaracy: 0.7944, DGCNN Test Accuracy: 0.7965\n",
            "\n",
            "Epoch: 20\n",
            "GAT Loss: 0.4775, GAT Train Accuaracy: 0.7763, GAT Test Accuracy: 0.7822\n",
            "DGCNN Loss: 0.4530, DGCNN Train Accuaracy: 0.7965, DGCNN Test Accuracy: 0.7948\n",
            "\n",
            "\n",
            "Training complete\n",
            "Best GAT Accuracy: 0.78545\n",
            "Best DGCNN Accuracy: 0.79655\n"
          ]
        }
      ],
      "source": [
        "def main(num_epochs):\n",
        "\n",
        "    device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')          #Checking if MPS (as I am training on a MacBook Pro) is available and using it if it is. Otherwise using CPU\n",
        "    print(device) #Can be used for checking what device being used for training\n",
        "    data_list=create_graph_list(k=16)                                                   #I am using k as 16 for k-NN\n",
        "    train_data,test_data=train_test_split(data_list, test_size=0.2, random_state=42)    #I have used a 80:20 split for training and testing data\n",
        "\n",
        "    train_loader=DataLoader(train_data, batch_size=32, shuffle=True)                    #I am doing batch processing here\n",
        "    test_loader=DataLoader(test_data, batch_size=32, shuffle=False)                     #Each Batch contains 32 graphs/jets and training data batches have been shuffled\n",
        "\n",
        "    in_channels=data_list[0].x.shape[1]                                                 #Number of input freatures per particle/node (i.e. d in (N,M,d))\n",
        "\n",
        "    gat_model=QG_GAT(in_channels, hidden_channels=64, out_channels=2, heads=4).to(device)   #Instance of our Quark Gluon GAT Classifier\n",
        "    dgcnn_model=QG_DGCNN(in_channels, hidden_channels=64, out_channels=2, k=16).to(device)  #Instance of the DGCNN Classifier\n",
        "\n",
        "    gat_optimizer=torch.optim.Adam(gat_model.parameters(), lr=1e-3)                            #I am using ADAM for training with 10^-3 learning rate\n",
        "    dgcnn_optimizer=torch.optim.Adam(dgcnn_model.parameters(), lr=1e-3)\n",
        "\n",
        "    gat_best_accuracy, dgcnn_best_accuracy=0,0                                         #Starting Training and Testing here over num_epochs\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "\n",
        "        gat_train_loss = train_epoch(gat_model, train_loader, gat_optimizer, device)\n",
        "        gat_train_accuracy = test_epoch(gat_model, train_loader, device)\n",
        "        gat_test_accuracy = test_epoch(gat_model, test_loader, device)\n",
        "\n",
        "        dgcnn_train_loss = train_epoch(dgcnn_model, train_loader, dgcnn_optimizer, device)\n",
        "        dgcnn_train_accuracy = test_epoch(dgcnn_model, train_loader, device)\n",
        "        dgcnn_test_accuracy = test_epoch(dgcnn_model, test_loader, device)\n",
        "\n",
        "        if gat_test_accuracy>gat_best_accuracy:\n",
        "            gat_best_accuracy=gat_test_accuracy\n",
        "            torch.save(gat_model.state_dict(), 'best_gat_model.pt')                    #Saving the best model parameters for later use. I can load into the model if I wish to\n",
        "\n",
        "        if dgcnn_test_accuracy>dgcnn_best_accuracy:\n",
        "            dgcnn_best_accuracy=dgcnn_test_accuracy\n",
        "            torch.save(dgcnn_model.state_dict(), 'best_dgcnn_model.pt')\n",
        "\n",
        "        print(f\"Epoch: {epoch+1:02d}\")\n",
        "        print(f\"GAT Loss: {gat_train_loss:.4f}, GAT Train Accuaracy: {gat_train_accuracy:.4f}, GAT Test Accuracy: {gat_test_accuracy:.4f}\")\n",
        "        print(f\"DGCNN Loss: {dgcnn_train_loss:.4f}, DGCNN Train Accuaracy: {dgcnn_train_accuracy:.4f}, DGCNN Test Accuracy: {dgcnn_test_accuracy:.4f}\")\n",
        "        print()\n",
        "\n",
        "    print(f\"\\nTraining complete\")\n",
        "    print(\"Best GAT Accuracy:\", gat_best_accuracy)\n",
        "    print(\"Best DGCNN Accuracy:\", dgcnn_best_accuracy)\n",
        "\n",
        "if __name__==\"__main__\":\n",
        "    main(num_epochs=20)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Task-3\n",
        "\n",
        "Inspired by my experience at the Yale Quantum Hackathon in April 2024, I spent that summer immersed in the IBM Quantum Platform and I started learning quantum computing. Since then, I have taken up coursework, done an internship project on Quantum Security, won two hackathons via quantum projects and I am also a research assistant at UConn exploring variational quantum algorithms like QAOA, and quantum key distribution protocols like BB84. Specifically in QAOA I am working with mixed binary optimization with ADMM formulations combined with QAOA. I have dived deep into the source code to do tasks such as sample filtering which is not available by default in the qiskit SDK. I am familiar with qiskit, cirq, and pennylane SDKs for development but I prefer qiskit and cirq as they seem more deterministic and intuitive to me.\n",
        "\n",
        "My experiences so far have made me strongly interested in hybrid quantum-classical methods and their applications in complex optimization and pattern recognition tasks, which makes me particularly excited about the Q-MAML project."
      ],
      "metadata": {
        "id": "KR9FyEUibqHF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Task 11: Installing Requirements"
      ],
      "metadata": {
        "id": "1OXH5PBs4jSI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#All modules installed previously\n",
        "#!pip install torch\n",
        "!pip install --quiet cirq"
      ],
      "metadata": {
        "id": "bHR545jt4sP9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3f99a051-a79b-47be-fdd6-bc10ab83980e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.6/45.6 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m30.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m532.7/532.7 kB\u001b[0m \u001b[31m26.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.5/60.5 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.3/69.3 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m596.5/596.5 kB\u001b[0m \u001b[31m26.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m47.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.6/294.6 kB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m203.2/203.2 kB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.5/6.5 MB\u001b[0m \u001b[31m57.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m31.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m51.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m44.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.7/117.7 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m739.1/739.1 kB\u001b[0m \u001b[31m28.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for rpcq (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Task 11: Required Imports"
      ],
      "metadata": {
        "id": "aEZv3x7v5Eyx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import cirq\n",
        "import math"
      ],
      "metadata": {
        "id": "Dot8KZBT5KYz"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Task-11: Code"
      ],
      "metadata": {
        "id": "Cs2wEspr-Ief"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hyperparameter Setup:"
      ],
      "metadata": {
        "id": "8KhrKFnk5lqX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_qubits = 4             #Using 4 qubits\n",
        "n_params = 3 * n_qubits  # Each qubit has 3 rotation angles\n",
        "n_samples = 50           # dataset size\n",
        "n_epochs = 20            # training on 20 epochs\n",
        "learning_rate = 0.02     # using lr = 0.02\n",
        "eps = 1e-3               # finite-difference step size\n",
        "\n",
        "input_dim = 5            # dimension of each input vector\n",
        "hidden_dim = 16\n",
        "output_dim = n_params    # MLP output: parameters for PQC"
      ],
      "metadata": {
        "id": "GFFABeSz5r0x"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "  Circuits:"
      ],
      "metadata": {
        "id": "Yl0O3E3z6adR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_circuit(params):\n",
        "    \"\"\"\n",
        "    Build a 4-qubit circuit in Cirq applying single-qubit rotations\n",
        "    and a chain of CNOTs, then measure the Z-expectation of qubit 0.\n",
        "    \"\"\"\n",
        "\n",
        "    qubits = [cirq.LineQubit(i) for i in range(n_qubits)]  # Creating 4 linequbits\n",
        "\n",
        "\n",
        "    reshaped = params.reshape(n_qubits, 3) # Converting params into shape (4, 3)\n",
        "    # Each qubit i has 3 angles: (rX, rY, rZ) or we can do cirq.Rot, but let's just do separate rotations\n",
        "\n",
        "    circuit = cirq.Circuit()\n",
        "\n",
        "    # Single-qubit rotations\n",
        "    for i in range(n_qubits):\n",
        "        rx_angle, ry_angle, rz_angle = reshaped[i]\n",
        "        circuit.append(cirq.rx(rx_angle)(qubits[i]))\n",
        "        circuit.append(cirq.ry(ry_angle)(qubits[i]))\n",
        "        circuit.append(cirq.rz(rz_angle)(qubits[i]))\n",
        "\n",
        "    # Chain of CNOTs for entangling\n",
        "    for i in range(n_qubits - 1):\n",
        "        circuit.append(cirq.CNOT(qubits[i], qubits[i + 1]))\n",
        "\n",
        "    return circuit, qubits\n",
        "\n",
        "def run_circuit(params):\n",
        "    \"\"\"\n",
        "    Construct and run the circuit, then return the expectation value of Z on qubit 0.\n",
        "    \"\"\"\n",
        "    circuit, qubits = build_circuit(params)\n",
        "\n",
        "\n",
        "    # We measure <Z> by appending a measurement in the computational basis in a simulation. For expectation using cirq's built in simulator\n",
        "    simulator = cirq.Simulator()\n",
        "    result = simulator.simulate(circuit)\n",
        "    final_state = result.final_state_vector   # final_state is a complex state vector of length 2^n_qubits\n",
        "\n",
        "    # Expectation of Z on qubit 0 for state |psi> = sum_k alpha_k |k>.\n",
        "    # |0>_Z -> amplitude indices that have 0 in the first qubit bit; |1>_Z -> those that have 1 in first qubit bit.\n",
        "    # In general: <psi|Z_0|psi> = sum_{k} |alpha_k|^2 * z_k, where z_k = +1 if qubit-0 bit is 0, else -1.\n",
        "\n",
        "    exp_val = 0.0\n",
        "    dim = 2 ** n_qubits\n",
        "    for idx in range(dim):\n",
        "        amp = final_state[idx]\n",
        "        prob = (amp.real**2 + amp.imag**2)\n",
        "        # bit of qubit0 is (idx & 1)\n",
        "        bit0 = (idx & 1)\n",
        "        z_val = 1.0 if bit0 == 0 else -1.0\n",
        "        exp_val += z_val * prob\n",
        "\n",
        "    return torch.tensor([exp_val], dtype=torch.float32)"
      ],
      "metadata": {
        "id": "vtfoVMz78Fno"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Custom Autograd Class for the Circuits:"
      ],
      "metadata": {
        "id": "sFwDknXM9GL9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CircuitFunction(torch.autograd.Function):\n",
        "    \"\"\"\n",
        "    A class that does forward pass by simulating the circuit,\n",
        "    and backward pass by finite-differences on each parameter.\n",
        "    \"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def forward(ctx, params):\n",
        "        # (we do require_grad in the code that calls us, but here we do normal forward)\n",
        "        params_ = params.detach().cpu().numpy() #detach and convert for cirq\n",
        "        value = run_circuit(params_)\n",
        "        ctx.save_for_backward(params)  # store for backward\n",
        "        return value  # shape [1]\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        \"\"\"\n",
        "        Finite-difference approximation:\n",
        "        derivative wrt param_i ~ (f(params + e_i*eps) - f(params - e_i*eps)) / (2*eps)\n",
        "        \"\"\"\n",
        "        (params,) = ctx.saved_tensors\n",
        "        params_np = params.detach().cpu().numpy()\n",
        "\n",
        "        grad_params = torch.zeros_like(params)\n",
        "        for i in range(len(params_np)):\n",
        "            # + eps\n",
        "            params_plus = params_np.copy()\n",
        "            params_plus[i] += eps\n",
        "            f_plus = run_circuit(params_plus)\n",
        "\n",
        "            # - eps\n",
        "            params_minus = params_np.copy()\n",
        "            params_minus[i] -= eps\n",
        "            f_minus = run_circuit(params_minus)\n",
        "\n",
        "            # partial derivative\n",
        "            dfdtheta_i = (f_plus - f_minus) / (2.0 * eps)\n",
        "            grad_params[i] = dfdtheta_i\n",
        "\n",
        "        # Multiply by incoming grad_output (chain rule)\n",
        "        grad_params = grad_output * grad_params\n",
        "        return grad_params\n",
        "\n",
        "\n",
        "def quantum_forward(params):\n",
        "    \"\"\"\n",
        "    Helper function that calls our custom autograd function.\n",
        "    \"\"\"\n",
        "    return CircuitFunction.apply(params)"
      ],
      "metadata": {
        "id": "pjmQ9PkV9SxG"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creating MLP:"
      ],
      "metadata": {
        "id": "YCSPjOYW9p86"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(input_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, output_dim)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.net(x)"
      ],
      "metadata": {
        "id": "e4q5Hbop9prD"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Generation and Training Setup:"
      ],
      "metadata": {
        "id": "kUn1PH3v-OL3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = torch.randn(n_samples, input_dim) # Sample random data from normal distribution\n",
        "y = torch.randn(n_samples, 1) # we want circuit output ~ some random values\n",
        "\n",
        "model = MLP(input_dim, hidden_dim, output_dim)\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "loss_fn = nn.MSELoss()"
      ],
      "metadata": {
        "id": "z9z9pnI5-Dt2"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training:"
      ],
      "metadata": {
        "id": "Io-xMi6o-lay"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(n_epochs):\n",
        "    total_loss = 0.0\n",
        "    for i in range(n_samples):\n",
        "        xi = X[i].unsqueeze(0)   # shape [1, input_dim]\n",
        "        yi = y[i]               # shape [1]\n",
        "\n",
        "        # 1) Forward MLP to get PQC parameters\n",
        "        pqc_params = model(xi)  # shape [1, n_params]\n",
        "        pqc_params = pqc_params.view(-1)  # flatten to [n_params]\n",
        "\n",
        "        # 2) Evaluate quantum circuit\n",
        "        out = quantum_forward(pqc_params) # shape [1]\n",
        "\n",
        "        # 3) Compute MSE\n",
        "        loss = loss_fn(out, yi)\n",
        "\n",
        "        # 4) Backprop\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    avg_loss = total_loss / n_samples\n",
        "    print(f\"Epoch [{epoch+1}/{n_epochs}] - Loss: {avg_loss:.6f}\")\n",
        "\n",
        "print(\"Training complete!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "9SYy2_RN-oA_",
        "outputId": "2e81d0b6-42da-48f1-efbe-ac2ba4f39feb"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/20] - Loss: 0.977558\n",
            "Epoch [2/20] - Loss: 0.894064\n",
            "Epoch [3/20] - Loss: 0.863842\n",
            "Epoch [4/20] - Loss: 0.874149\n",
            "Epoch [5/20] - Loss: 0.832021\n",
            "Epoch [6/20] - Loss: 0.870231\n",
            "Epoch [7/20] - Loss: 0.869434\n",
            "Epoch [8/20] - Loss: 0.886635\n",
            "Epoch [9/20] - Loss: 0.846890\n",
            "Epoch [10/20] - Loss: 0.863747\n",
            "Epoch [11/20] - Loss: 0.857026\n",
            "Epoch [12/20] - Loss: 0.836613\n",
            "Epoch [13/20] - Loss: 0.824598\n",
            "Epoch [14/20] - Loss: 0.809233\n",
            "Epoch [15/20] - Loss: 0.798381\n",
            "Epoch [16/20] - Loss: 0.828946\n",
            "Epoch [17/20] - Loss: 0.812880\n",
            "Epoch [18/20] - Loss: 0.807946\n",
            "Epoch [19/20] - Loss: 0.783687\n",
            "Epoch [20/20] - Loss: 0.855771\n",
            "Training complete!\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}